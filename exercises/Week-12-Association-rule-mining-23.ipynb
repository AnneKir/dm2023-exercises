{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises Week 12 - Association Rule Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Local imports\n",
    "import sys\n",
    "sys.path.append('./utilities')\n",
    "from load_data import load_market_basket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.\n",
    "\n",
    "The work you do in this exercise will be very useful also in your hand-in.\n",
    "\n",
    "We learned the Apriori algorithm in class. Make your own implementation. \n",
    "\n",
    "We will use the anonymized real-world retail market basket data from: http://fimi.ua.ac.be/data/.\n",
    "\n",
    "This data comes from an anonymous Belgian retail store, and was donated by Tom Brijs from Limburgs Universitair Centrum, Belgium. The original data contains 16,470 different items and 88,162 transactions. You may only work with the top-50 items in terms of occurrence frequency.\n",
    "\n",
    "Your task is to:\n",
    "1. Implement the Apriori algorithm.\n",
    "2. Apply the Apriori algorithm on these data to find association rules with minimum support 0.05 and minimum confidence 0.7. Write down the rules in descending order of confidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the retail data\n",
    "transactions = load_market_basket()\n",
    "\n",
    "def book_example():\n",
    "    return [\n",
    "        [1, 2, 4, 5],\n",
    "        [2, 3, 5],\n",
    "        [1, 2, 4, 5],\n",
    "        [1, 2, 3, 5],\n",
    "        [1, 2, 3, 4, 5], \n",
    "        [2, 3, 4],\n",
    "    ]\n",
    "\n",
    "def filter_transactions(T, k=50):\n",
    "    \"\"\"\n",
    "        Keep only the top k items in the transactions.\n",
    "        Remove transactions that become empty.\n",
    "    \"\"\"\n",
    "    # Count occurences of each item\n",
    "    counts = [0] * 16470\n",
    "    for t in T:\n",
    "        for i in t:\n",
    "            counts[i] += 1\n",
    "\n",
    "    # Sort and select top k\n",
    "    counts = np.array(counts)\n",
    "    order  = np.argsort(counts)[::-1] # reverse the sorted order\n",
    "\n",
    "    indexes_to_keep = order[:k]       # Keep the top k items\n",
    "    index_set = set(indexes_to_keep)  # Convert to python set for efficiency\n",
    "\n",
    "    # Filter transactions\n",
    "    T_new = [t_ for t_ in  [list(filter(lambda i: i in index_set, t)) for t in T]  if t_]\n",
    "    return T_new\n",
    "\n",
    "T = filter_transactions(transactions, k=50)\n",
    "#T = book_example() # You can first use the very small dataset to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, chain\n",
    "\n",
    "# Tiny function for generating rules from tuples\n",
    "# Ex: rule((1, 2), (5)) outputs \"(1, 2) => (5)\"\n",
    "rule  = lambda lhs, rhs: \"%s => %s\" % (str(lhs), str(rhs)) # For generating rule strings\n",
    "\n",
    "\n",
    "def compute_support(Ck, T, still_applicable=None):\n",
    "    counts = None\n",
    "    ### TODO Your code here\n",
    "    if still_applicable is None: still_applicable = [True] * len(T)\n",
    "    \n",
    "    # C1 = { x:0 for l in Ck for x in l}\n",
    "    # for list1 in T:\n",
    "    #     for i in list1:\n",
    "    #         C1[i] += 1 #count\n",
    "    \n",
    "\n",
    "    counts = {}\n",
    "    for row, t in enumerate(T):\n",
    "        if not still_applicable[row]: continue #skip row if not applicable\n",
    "        #flag for sayign if found anything still a subset\n",
    "        found_any = False\n",
    "        for c in Ck: #ck should be list of tuples\n",
    "            if set.issubset(set(c),t):\n",
    "                #increase if in dictionary else add\n",
    "                if c in counts: counts[c] +=1\n",
    "                else: counts[c] = 1\n",
    "                found_any = True\n",
    "        still_applicable[row] = found_any #if we found something then it is still usefull\n",
    "\n",
    "\n",
    "    ### TODO Your code here\n",
    "    return counts\n",
    "\n",
    "def extend_prefix_tree(Ck_prev):\n",
    "    Ck = None\n",
    "    ### TODO Your code here\n",
    "    Ck = set()\n",
    "    #by making set we can ignore stuff with checking if we add twice ?\n",
    "\n",
    "    #join step\n",
    "    for itemset in Ck_prev:\n",
    "        its1 = tuple(sorted(itemset)) #sort and make tuple\n",
    "        for itemset2 in Ck_prev:\n",
    "            #we want lex sorted such that we can use slicing\n",
    "            its2 = tuple(sorted(itemset2))\n",
    "            #check if same uptii k-1 and that last is not same\n",
    "            if its1[:-1] == its2[-1] and its1[-1] < its2[-1]: # its1>its2 would in the end do the same\n",
    "                # its1 and its2 are a tuple, \n",
    "                Ck.add(its1 + its2[-1:])\n",
    "                #could have cast them to sets, used union, and then casted them back to tuplles\n",
    "    #now prune\n",
    "    to_remove = set()\n",
    "    for c in Ck:\n",
    "        for subset in combinations(c, len(c)-1): #every comb execpt c?\n",
    "            if not subset in Ck_prev:\n",
    "                to_remove.add(c)\n",
    "                break #we dont need to check other subsets\n",
    "\n",
    "    for c in to_remove:\n",
    "        Ck.remove(c)\n",
    "\n",
    "    ### TODO Your code here\n",
    "    return Ck #now we have all sets where all subsets are present?\n",
    "\n",
    "def powerset(iterable):\n",
    "    ### TODO Your code here\n",
    "    s = list(iterable)\n",
    "    # we want any sets at most the lengt of the list\n",
    "    return chain.from_iterable(combinations(s,r) for r in range(1, len(s)))\n",
    "    ### TODO Your code here\n",
    "\n",
    "def apriori_algorithm(T, support=0.05, min_confidence=0.7):\n",
    "    \"\"\"\n",
    "        Apriori algorithm for mining association rules.\n",
    "        Inputs:\n",
    "            T:               A list of lists, each inner list will contiain integer-item-ids. \n",
    "                             Example: T = [[1, 2, 5], [2, 3, 4], [1, 6]]\n",
    "            support:         The proportion of occurences needed to keep itemsets.\n",
    "            min_confidence:  Minimum confidence for the algorithm to output the rule.\n",
    "        \n",
    "        Outputs:\n",
    "            rules:           List of tuples [(rule:str, confidence:float), ... ]\n",
    "                             Example: [(\"(1, 2) => (5)\", 0.84), (\"(3, 4) => (7)\", 0.75)]\n",
    "    \"\"\"\n",
    "    rules = []\n",
    "    ### TODO Your code here\n",
    "    n = len(T)\n",
    "    \n",
    "    itemsets = {}\n",
    "    C1 = set()\n",
    "    for t in T:\n",
    "        for ti in t: C1.add((ti,)) #add all items as a tuple\n",
    "\n",
    "    still_applicable = [True] * n\n",
    "    Ck = C1\n",
    "\n",
    "    k=1\n",
    "    while Ck:\n",
    "        itemsets[k] = compute_support(Ck, T, still_applicable)\n",
    "        Ck_copy = Ck.copy()\n",
    "        for itemset in Ck:\n",
    "            #\n",
    "            if itemsets[k][itemset] / n < support:\n",
    "                del itemsets[k][itemset]\n",
    "                Ck_copy.remove(itemset)\n",
    "        Ck = extend_prefix_tree(Ck_copy) #want to look at the next itemsets of k+1\n",
    "        #terminates at some point because we prune and stuff\n",
    "        k+=1\n",
    "    \n",
    "    k=2 #we dont care about rules of size 1\n",
    "    rules= []\n",
    "\n",
    "    while k <= max(itemsets.keys()):\n",
    "        for itemset in itemsets[k].keys():\n",
    "            for rhs in powerset(itemset):\n",
    "                remaining = set(itemset).difference(set(rhs))\n",
    "                lhs = tuple(sorted(remaining))\n",
    "\n",
    "                conf = itemsets[k][itemset] / itemsets[len(lhs)][lhs]\n",
    "                if conf >= min_confidence:\n",
    "                    rules.append((rule(lhs,rhs),conf))\n",
    "\n",
    "        k +=1\n",
    "        # Use Lk to generate candidates of size k+1\n",
    "\n",
    "    ### TODO Your code here\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can try your algorithm. If you want, you could try changing the support and min_confidence to see what happens to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conf.    \t Rule\n"
     ]
    }
   ],
   "source": [
    "rules = apriori_algorithm(T, support=0.05, min_confidence=0.7)\n",
    "rules = sorted(rules, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"%-8s \\t %s\" % (\"Conf.\", \"Rule\"))\n",
    "for r in rules:\n",
    "    print(\"%7.4f%% \\t %s\" % r[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.\n",
    "\n",
    "We have learned how to mine frequent itemsets and association rules from a transaction database where each transaction consists of a simple set of items. You are asked to propose a framework for mining association rules from transaction data, in which each item in a transaction is associated with an integer number that counts how many times the items appears in the transaction. In a market basket, this count number indicates the number of copies of a product in a customer’s basket. For example, we do not only care whether a customer bought fish or not, but how many pieces of fish they bought.\n",
    "\n",
    "1. Define (extend) the notion of an itemset and an association rule in the case of such data.\n",
    "2. Describe an efficient algorithm that mines itemsets and association rules as defined in Exercise 2. Illustrate the pruning strategies used in your algorithm and explain how they relate to the Apriori principle.\n",
    "3. Extend your implementation of the Apriori algorithm to handle this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO your code here\n",
    "# You can copy pase the code from above and adjust it\n",
    "### TODO your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep dictionary for the items such that we also have count  \n",
    "Need to not just looup if item is present or not, but also how many then  \n",
    "  \n",
    "Differentiate between (A,1) => (B,1) and (A,2) => (B,3)  \n",
    "We still want to keep some conection between A's and B's \n",
    "  \n",
    "((A,2),(C,3) => (D,1)) different from (A,1),(C,3) is very strict, maybe limiting  \n",
    "\n",
    "Could also change notion of support. E.g if we have rule AC => D and have (A,3) (C,5) (D,4) would then support that rule 3 times.  \n",
    "\n",
    "Could also compute mean of all support of transactions  \n",
    "The mean of A2C3 => D1 and A1C3=>D2 would be A1.5 C3 => D1.5\n",
    "Can use mean to estimate how many items in the end?... \n",
    "Would end with \n",
    "Doesn't need to change the underlying algorithm, just need to also keep track of mean.\n",
    "Would loose on the threshold? Buying for large party would skew?  \n",
    "For an idividual perspective the mean would be useless.  \n",
    "\n",
    "\n",
    "\n",
    "What are our use-cases. <- the thing it all comes down to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Consider a dataset of transactions $D$, and let $D'$ be a dataset derived from $D$ by independently deleting items from transactions in $D$. In particular, any item in any transaction in $D$ is deleted with probability $p$.\n",
    "\n",
    "1. Given an itemset $S$, compute its expected support in $D'$ as a function of its support in $D$.\n",
    "2. Compute the probability that an itemset $S$, which is frequent in $D$, is also frequent in $D'$, under the same minimum support threshold.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(I) = |{t\\in T | I \\subseteq t(D)}|$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Hvis vi har {{A,B},{A,B},{A}} og det sidste A slettes så stiger support for reglen A=>B  \n",
    "\n",
    "Hvis vi har {{A,B},{A,B},{A}} og det sidste A slettes falder support for A og support for B er uændret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$|s|=k$  \n",
    "\n",
    "$T_i$\n",
    "\n",
    "$s\\leq i(t_i)$\n",
    "\n",
    "We need to figure out the prop of not being deleted.\n",
    "$Pr[not deleting any x \\in s from T_i]$\n",
    "\n",
    "$\\Pi_{x\\in s} (1-p) = (1-p)^k$\n",
    "\n",
    "$E[] = \\sum_{t_i \\in D} (1-p)^k*1 = (1-p)^k * S_{n,p?}(S,D)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second is bionominal distribution, but otherwise much of the same principle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
